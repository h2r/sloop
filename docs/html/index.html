<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="Welcome to the site SLOOP!">
        <title>SLOOP: Spatial Language Understanding for Object Search in Partially Observed City-scale Environments</title>

        <!-- Bootstrap -->
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

        <!-- Custom Style -->
        <link rel="stylesheet" href="_static/main.css"/>

        <!-- Icon -->
        <link rel="icon" href="_static/favicon.ico" type="image/x-icon" />

    </head>

    <body>
        <div class="container" style="margin-top:5em; margin-bottom:5em">
            <div class="row justify-content-center">
                <div class="col-xs-11 col-sm-10 col-md-9 col-lg-8">
                    <h2><a href="https://arxiv.org/abs/2012.02705"><strong>Spatial Language Understanding for Object Search in Partially Observed City-scale Environments</strong></a></h2>
<p><a href="https://kaiyuzheng.me/">Kaiyu Zheng</a>,
<a href="https://bayazitdeniz.github.io/">Deniz Bayazit</a>,
<a href="https://github.com/beckymathew">Rebecca Mathew</a>,
<a href="http://cs.brown.edu/people/epavlick/">Ellie Pavlick</a>,
<a href="https://h2r.cs.brown.edu/">Stefanie Tellex</a> <br>
Department of Computer Science, Brown University<br>
30th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN), 2021</p>
<p><strong>Github repository:</strong> <a href="https://github.com/h2r/sloop">https://github.com/h2r/sloop</a></p>
<p><strong>ArXiv:</strong> <a href="https://arxiv.org/abs/2012.02705">https://arxiv.org/abs/2012.02705</a></p>
<h2>Table of Contents:</h2>
<ul>
<li><a href="#installation">Installation</a></li>
<li><a href="#dataset-and-models">Download Dataset &amp; Models</a></li>
<li><a href="#results">Download Results &amp; Reproduce</a></li>
<li><a href="#openstreetmap-demo">OpenStreetMap Demo</a></li>
<li><a href="#airsim-demo">AirSim Demo</a></li>
<li><a href="#citation">Citation</a></li>
</ul>
<h2>Talk &amp; Demo</h2>
<div class="row ml-2 mt-3 mb-5">
<iframe width="560" height="315" src="https://www.youtube.com/embed/TNwi1kS715Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

<div class="row ml-2 mt-3 mb-5">
<iframe width="560" height="315" src="https://www.youtube.com/embed/V54RY8v8VmA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

<h2>1. Installation <a name="installation"/></h2>
<p>The required python version is Python 3.6+.</p>
<ol>
<li>
<p>Clone the repository and create and virtual environment with the following lines.</p>
<pre><code>git clone git@github.com:h2r/sloop.git
cd sloop;
virtualenv -p $(which python3) venv/sloop
source venv/sloop/bin/activate</code></pre>
<p>For the last step, you can also do <code>source setup.bash</code> for convenience.</p>
</li>
<li>
<p>Install <a href="https://github.com/h2r/pomdp-py">pomdp-py</a></p>
<pre><code>pip install pomdp-py&gt;=1.2.4.5</code></pre>
</li>
<li>
<p>Install the <code>sloop</code> package. Assume you're at the root of the sloop repository.</p>
<pre><code>pip install -e .</code></pre>
<p>Note that this will install a number of dependencies, including <a href="https://h2r.github.io/pomdp-py/html/">pomdp-py</a> version 1.2.4.5. See <code>setup.py</code> for the list of packages. The <code>&gt;=</code> symbol assumes backwards compatibility of those packages.</p>
</li>
<li>
<p>Download spaCy model. For dependency parsing, we use the <a href="https://spacy.io/models/en#en_core_web_md"><code>en_core_web_md</code> model</a> from spaCy.
   You can download it by:</p>
<pre><code>python -m spacy download en_core_web_md</code></pre>
</li>
</ol>
<h2>2. Download Dataset &amp; Models <a name="dataset-and-models"/></h2>
<p>There is one dataset and two models.</p>
<ul>
<li>
<p>The dataset contains OpenStreetMap data and AMT spatial language descriptions and annotations.
  Download the dataset from  <a href="https://drive.google.com/file/d/1K1SRR3rHcM8Jndjhb-YTB5kqefDNYYbH/view?usp=sharing">here</a> (SL_OSM_Dataset, 3.6MB), and place it under <code>sloop/datasets</code> and extract there.
   After extraction your directory structure should look like:</p>
<pre><code>/ # repository root
  sloop/
      ...
      datasets/
        SL_OSM_Dataset/
          amt/
          frame_of_ref/
          ...</code></pre>
<p>Check out <a href="https://github.com/h2r/sloop/wiki/Dataset-Documentation">this wiki page</a> for documentation about the dataset.</p>
</li>
<li>
<p>The models are the frame of reference prediction models. There is a <strong>front</strong> model (for <em>front</em> and <em>behind</em>) and a <strong>left</strong> model (for <em>left</em> and <em>right</em>).
Download the models from <a href="https://drive.google.com/file/d/1XfOUa0xtRstUxJHBdNmk4SLJw970-4vV/view?usp=sharing">here</a> (models.zip, 42.4MB)
and place it under <code>sloop/oopomdp/experiments/resources</code>.</p>
<p>After extraction your directory structure should look like:</p>
<pre><code>/ # repository root
   sloop/
       ...
       oopomdp/
           experiments/
               resources/
                   models/
                       iter2_ego-ctx-foref-angle:front:austin
                       ...</code></pre>
</li>
</ul>
<h2>3. Download and Process Results <a name="results"/></h2>
<p>You can download the full results (including individual trial results) from <a href="https://drive.google.com/file/d/1Sg3Or5tB5Gublmv2okcWA4-mjIIwS5Oo/view?usp=sharing">here</a>
(all-joint-sloop.zip, 2.0GB), and place it under <code>sloop/</code></pre>
<p>After extraction your directory structure should look like:</p>
<pre><code>/ # repository root
  sloop/
      results/
          all-joint-sloop/
              langprior-austin_00_informed#5-austin-laser:fov=90:min*range=1:max*range=3:angle*increment=0.5:occlusion*enabled=False-na/
              ...
</code></pre>
<h4>To process results:</h4>
<ol>
<li><code>cd sloop/results/all-joint-sloop</code></li>
<li>
<pre><code>python gather_results.py</code></pre>
<p>Expected output</p>
<pre><code>...
 Warning: &lt;class 'sloop.oopomdp.experiments.reward_result.RewardsResult'&gt; result file rewards.yaml not found in /media/kz-wd-ssd/repo/sloop/results/all-joint-sloop/langprior-austin_380_rule#based#ego&gt;ctx&gt;foref&gt;angle-austin-laser:fov=90:min*range=1:max*range=4:angle*increment=0.5:occlusion*enabled=False-ego&gt;ctx&gt;foref&gt;angle
 Collected results in langprior-austin_380_rule#based#ego&gt;ctx&gt;foref&gt;angle-austin-laser:fov=90:min*range=1:max*range=4:angle*increment=0.5:occlusion*enabled=False-ego&gt;ctx&gt;foref&gt;angle
 Collected results in langprior-cleveland_41_mixture#full#auto-cleveland-laser:fov=90:min*range=1:max*range=3:angle*increment=0.5:occlusion*enabled=False-ego&gt;ctx&gt;foref&gt;angle
 Collected results in langprior-honolulu_19_informed#5-honolulu-laser:fov=90:min*range=1:max*range=3:angle*increment=0.5:occlusion*enabled=False-na
 Collected results in langprior-honolulu_35_keyword-honolulu-laser:fov=90:min*range=1:max*range=3:angle*increment=0.5:occlusion*enabled=False-na
 Collected results in langprior-washington,dc_33_keyword-washington,dc-laser:fov=90:min*range=1:max*range=4:angle*increment=0.5:occlusion*enabled=False-na
 Warning: &lt;class 'sloop.oopomdp.experiments.states_result.StatesResult'&gt; result file states.pkl not found in /media/kz-wd-ssd/repo/sloop/results/all-joint-sloop/langprior-austin_380_rule#based#ego&gt;ctx&gt;foref&gt;angle-austin-laser:fov=90:min*range=1:max*range=4:angle*increment=0.5:occlusion*enabled=False-ego&gt;ctx&gt;foref&gt;angle
 Collected results in langprior-cleveland_39_keyword#auto-cleveland-laser:fov=90:min*range=1:max*range=5:angle*increment=0.5:occlusion*enabled=False-na
 Collected results in langprior-denver_29_informed#5-denver-laser:fov=90:min*range=1:max*range=4:angle*increment=0.5:occlusion*enabled=False-na
 Collected results in langprio
 ...</code></pre>
<p>This will generate many plots and results in png/csv/yaml/json formats.</p>
</li>
<li>
<pre><code>python ../refine.py ./</code></pre>
<p>This will generate some more tables that are better organized and readable,
using the files generated from the last step as input.</p>
</li>
</ol>
<h3>To reproduce the experiment</h3>
<ol>
<li><code>cd sloop/results/all-joint-sloop</code></li>
<li>Notice there are several scripts <code>run_{1,2,3,4}.sh</code>. Each contains commands to run individual trials. You can break them up further if you want more parallelization (you can automatically break them up using the <code>generate_run_scripts.py</code> and increase the value for the variable <code>SPLIT</code>.</li>
<li>Now, run the experiment by running <code>./run_#.sh</code> wher <code>#</code> is the index of the run file.
Note that because the experiment is random, the final result may differ slightly from those reported in the paper.</li>
</ol>
<h2>4. Running on OpenStreetMap <a name="openstreetmap-demo"/></h2>
<p>You can now start a demo of spatial language object search on an OpenStreetMap by running</p>
<pre><code>cd sloop/oopomdp/experiments
python interface.py
</code></pre>
<p>This starts a terminal interface. We will walk through an example below.</p>
<p>At start, the program loads several things:</p>
<pre><code>$ python interface.py
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
Loading spacy model...
Loading spatial keywords...
Loading symbol to synonyms...
</code></pre>
<p>Then, it asks for a city name. You can enter any one from <code>austin, cleveland, denver, honolulu, washington_dc</code>.
We will enter <code>austin</code>.</p>
<pre><code>map name: austin
Loading mapinfo
</code></pre>
<p>Then, it asks for number of objects (maximum 3). We will enter 1:</p>
<pre><code>num objects, max 3 [2]: 1
</code></pre>
<p>Then, sensor range; The is the depth of the fan-shaped sensor with fixed field of view angle of 90 degrees.
Sensor range of 3 refers to a range of 15m, 4 is 20m, 5 is 25m.</p>
<pre><code>Sensor range [4]: 3
</code></pre>
<p>You can enter the true x, y location of the target object (a red car). Leave it blank for random.</p>
<pre><code>x, y for object R [random]:
</code></pre>
<p>Then, a window pops up. The blue circle is the robot. Its starting location is random.</p>
<p><img src="assets/start.png" width="400px"></p>
<p>Then, You are asked to enter a spatial language description of the target's location.</p>
<pre><code>Hint: the red car is by the lavaca street behind HiLo.
</code></pre>
<p>Now you can enter the method that essentially interprets the language as a prior belief over the target location,
through one belief update step. The choices are <code>mixture</code>, <code>sloop</code>, <code>keyword</code>, <code>informed</code>, <code>uniform</code>.
They correspond to SLOOP(m=4), SLOOP, MOS(keyword), informed, uniform in our experiments.</p>
<pre><code>Prior type [mixture]:
Loading ego_ctx_foref_angle model for right
...
</code></pre>
<p>Then the program processes the language, and then the robot starts to search. You will see
output similar to this</p>
<pre><code>...
Language: &quot;the red car is by the lavaca street behind HiLo.&quot;
MAP: austin
{'entities': ['RedHonda', 'HiLo', 'LavacaSt'], 'relations': [('RedHonda', 'LavacaSt', None), ('RedHonda', 'HiLo', 'behind')], 'lang': 'the RedHonda is by LavacaSt behind HiLo.'}
2021-07-07 13:49:44.438213 Event (Normal): Trial interface-mixture_mixture-austin-laser:fov=90:min*range=1:max*range=3:angle*increment=0.5:occlusion*enabled=False-ego&gt;ctx&gt;foref&gt;angle | Step 1:  action: move-vw-TurnRight   reward: -11.000   cum_reward: -11.000   NumSims: 300   PlanTime: 0.61667
2021-07-07 13:49:45.094287 Event (Normal): Trial interface-mixture_mixture-austin-laser:fov=90:min*range=1:max*range=3:angle*increment=0.5:occlusion*enabled=False-ego&gt;ctx&gt;foref&gt;angle | Step 2:  action: move-vw-TurnRight   reward: -11.000   cum_reward: -22.000   NumSims: 300   PlanTime: 0.61652
2021-07-07 13:49:45.781581 Event (Normal): Trial interface-mixture_mixture-austin-laser:fov=90:min*range=1:max*range=3:angle*increment=0.5:occlusion*enabled=False-ego&gt;ctx&gt;foref&gt;angle | Step 3:  action: move-vw-TurnRight   reward: -11.000   cum_reward: -33.000   NumSims: 300   PlanTime: 0.64700
2021-07-07 13:49:46.453592 Event (Normal): Trial interface-mixture_mixture-austin-laser:fov=90:min*range=1:max*range=3:angle*increment=0.5:occlusion*enabled=False-ego&gt;ctx&gt;foref&gt;angle | Step 4:  action: move-vw-Forward   reward: -11.000   cum_reward: -44.000   NumSims: 300   PlanTime: 0.63122
2021-07-07 13:49:47.136918 Event (Normal): Trial interface-mixture_mixture-austin-laser:fov=90:min*range=1:max*range=3:angle*increment=0.5:occlusion*enabled=False-ego&gt;ctx&gt;foref&gt;angle | Step 5:  action: find   reward: 1000.000   cum_reward: 956.000   NumSims: 300   PlanTime: 0.65474
2021-07-07 13:49:47.164189 Event (Normal): Trial %s | Task Finished!
...
</code></pre>
<p>and the window now may look something like this:</p>
<p><img src="assets/searching.png" width="400px"></p>
<h2>5. Running on AirSim <a name="airsim-demo"/></h2>
<p>This requires installation of Unreal Engine 4 and AirSim, following AirSim's documentation. (<a href="https://microsoft.github.io/AirSim/build_linux/">Linux</a>, <a href="https://microsoft.github.io/AirSim/build_windows/">Windows</a>),
as well as downloading assets for the UE4 environment e.g. <a href="https://www.unrealengine.com/marketplace/en-US/product/modular-neighborhood-pack">Neighborhoods</a> and <a href="https://www.unrealengine.com/marketplace/en-US/product/urban-city">Urban City</a>.
Detailed instructions can be found in <a href="https://github.com/h2r/sloop/wiki/Running-SLOOP-object-search-on-AirSIm-UE4">this wiki page</a>.</p>
<p>The search trial is started by running <code>python controller.py</code>.</p>
<p>Here is an example screenshot:</p>
<p><img src="https://i.imgur.com/djFTbVu.png" width="600px"></p>
<h2>6. Citation <a name="citation"/></h2>
<pre><code>@inproceedings{sloop-roman-2020,
  title={Spatial Language Understanding for Object Search in Partially Observed Cityscale Environments},
  author={Zheng, Kaiyu and Bayazit, Deniz and Mathew, Rebecca and Pavlick, Ellie and Tellex, Stefanie},
  booktitle={2021 IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},
  year={2021},
  organization={IEEE}
}
</code></pre>
                </div>
            </div>
            <footer class="footer text-justify">
                <div class="row justify-content-center mt-1">
                    <a href="http://h2r.cs.brown.edu/">Humans to Robots Lab</a>
                </div>
                <div class="row justify-content-center">
                    Brown University
                </div>
                <div class="row justify-content-center mt-2">
                    <a href="https://brown.edu/"><img src="assets/brownlogo.png" width="30px"></a>
                </div>
            </footer>
        </div>
    </body>
</html>
